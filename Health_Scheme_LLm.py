# -*- coding: utf-8 -*-
"""Copy of Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11T43ck4ry8rJcqjJPL7DDX8f9SoZQ-Ql
"""

!pip install langchain
!pip install bitsandbytes accelerate transformers
!pip install datasets loralib sentencepiece
!pip install pypdf
!pip install sentence-transformers

!pip install unstructured

!pip install tokenizers

!pip install xformers

!pip install pinecone-client

!pip install langchain
!pip install langchain-community
import langchain
import langchain_community

from langchain.document_loaders import UnstructuredURLLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.vectorstores import Pinecone
import pinecone
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from huggingface_hub import notebook_login
import textwrap
import sys
import os
import torch

!pip install nltk
import nltk

# Download the punkt tokenizer and POS tagger data
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')

URLs = [
    "https://wellnessdestinationindia.com/success-story/medical-facilities-in-india",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC5144115/",
    "https://pib.gov.in/pressreleaseshare.aspx?prid=1576128",
    "https://www.cancerassist.in/govt-schemes-for-cancer-treatment-",
    "https://www.magicinepharma.com/blogs/central-government-aids-cancer-treatment-india",
    "https://dghs.gov.in/content/1353_3_NationalOrganTransplantProgramme.aspx",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC7480502/"
]

loader = UnstructuredURLLoader(urls=URLs)

# Load documents from URLs
data = loader.load()

# Print the first document
print(data)

len(data)

text_splitter = CharacterTextSplitter(separator="\n", chunk_size=1000, chunk_overlap=200)

# Split the text
text_chunks = text_splitter.split_documents(data)

len(text_chunks)

text_chunks[0]

text_chunks[1]

text_chunks[10]

# List of URLs (with proper commas separating each URL)
URLs = [
    "https://wellnessdestinationindia.com/success-story/medical-facilities-in-india",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC5144115/",
    "https://pib.gov.in/pressreleaseshare.aspx?prid=1576128",
    "https://www.cancerassist.in/govt-schemes-for-cancer-treatment-",
    "https://www.magicinepharma.com/blogs/central-government-aids-cancer-treatment-india",
    "https://dghs.gov.in/content/1353_3_NationalOrganTransplantProgramme.aspx",
    "https://pmc.ncbi.nlm.nih.gov/articles/PMC7480502/"
]

# Load documents from all URLs using UnstructuredURLLoader
loader = UnstructuredURLLoader(urls=URLs)

# Load the data
data = loader.load()

# Print the number of documents loaded
print(f"Number of documents loaded: {len(data)}")

# Print the first document
print("First document content:")
print(data[0].page_content)

# Initialize the text splitter
text_splitter = CharacterTextSplitter(separator="\n", chunk_size=1000, chunk_overlap=200)

# Split all the loaded documents into chunks
text_chunks = text_splitter.split_documents(data)

# Print the number of chunks created
print(f"Number of chunks created: {len(text_chunks)}")

# Print the first chunk
print("First chunk content:")
print(text_chunks[0])

embeddings=HuggingFaceEmbeddings()

embeddings

query_result=embeddings.embed_query(text_chunks[0].page_content)

len(query_result)

query_result

!pip install pinecone

from pinecone import Pinecone, ServerlessSpec

pc = Pinecone(api_key="")

index_name = "llamaprojects"

index = pc.Index("llamaprojects")

index

!pip install --upgrade pinecone-client
!pip install --upgrade langchain

#import os

# Replace 'YOUR_API_KEY' with your actual Pinecone API key
os.environ["PINECONE_API_KEY"] = ""

# Assuming 'index' is your Pinecone index object
# and 'embeddings' is your HuggingFaceEmbeddings object
from langchain.vectorstores import Pinecone
vectorstore = Pinecone.from_texts(
    texts=[t.page_content for t in text_chunks],
    embedding=embeddings,
    index_name=index_name
)

!pip install langchain-huggingface

from langchain_huggingface import HuggingFaceEndpoint

from google.colab import userdata
userdata.get('huggingFace')

import os
os.environ["HUGGINGFACEHUB_API_TOKEN"]=""

model= "meta-llama/Llama-2-7b-chat-hf"

from huggingface_hub import notebook_login

notebook_login()

model= "meta-llama/Llama-2-7b-chat-hf"

tokenizer = AutoTokenizer.from_pretrained(model,use_auth_token=True)

model = AutoModelForCausalLM.from_pretrained(model,
                                             device_map="auto",
                                             torch_dtype=torch.float16,
                                             use_auth_token=True,
                                             load_in_8bit=True)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    torch_dtype=torch.float16,
    device_map="auto",
    max_new_tokens = 512,
    do_sample=True,
    top_k=30,
    num_return_sequences=1,
    eos_token_id=tokenizer.eos_token_id
)

from langchain_huggingface import HuggingFacePipeline
llm = HuggingFacePipeline(pipeline = pipe,model_kwargs = {'temperature':0})

llm.predict("What Healthcare Schemes are available?")

